{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9S_E5AgNJkNA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bS6MNMEzKBxs"
      },
      "outputs": [],
      "source": [
        "# Initialising the CNN\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# CONVOLUTION LAYER\n",
        "input_size = (128, 128)\n",
        "model.add(tf.keras.layers.Convolution2D(32, 3, 3, input_shape = (*input_size, 3), activation = 'relu'))\n",
        "\n",
        "# POOLING LAYER\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# CONVOLUTION LAYER\n",
        "model.add(tf.keras.layers.Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# FLATTENING LAYER\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# FULLY CONNECTED LAYER\n",
        "model.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbrjo20M6CXH"
      },
      "source": [
        "## COMPILING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4jwlI36vMvJW"
      },
      "outputs": [],
      "source": [
        "# Compiling the CNN\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CWYiHqn6SiJ"
      },
      "source": [
        "## FITTING THE CNN TO THE IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggxWEo0-M1h3",
        "outputId": "04083df2-faf7-4779-c830-12ec5155671f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2008 images belonging to 2 classes.\n",
            "Found 392 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "31/31 [==============================] - 188s 6s/step - loss: 0.6966 - accuracy: 0.5226 - val_loss: 0.6994 - val_accuracy: 0.4554\n",
            "Epoch 2/25\n",
            "31/31 [==============================] - 82s 3s/step - loss: 0.6940 - accuracy: 0.5098 - val_loss: 0.6922 - val_accuracy: 0.4911\n",
            "Epoch 3/25\n",
            "31/31 [==============================] - 45s 1s/step - loss: 0.6945 - accuracy: 0.4971 - val_loss: 0.6904 - val_accuracy: 0.5625\n",
            "Epoch 4/25\n",
            "31/31 [==============================] - 26s 837ms/step - loss: 0.6924 - accuracy: 0.5176 - val_loss: 0.6917 - val_accuracy: 0.5179\n",
            "Epoch 5/25\n",
            "31/31 [==============================] - 19s 602ms/step - loss: 0.6911 - accuracy: 0.5225 - val_loss: 0.6877 - val_accuracy: 0.5134\n",
            "Epoch 6/25\n",
            "31/31 [==============================] - 16s 521ms/step - loss: 0.6900 - accuracy: 0.5527 - val_loss: 0.6845 - val_accuracy: 0.5357\n",
            "Epoch 7/25\n",
            "31/31 [==============================] - 15s 482ms/step - loss: 0.6836 - accuracy: 0.5413 - val_loss: 0.6898 - val_accuracy: 0.5357\n",
            "Epoch 8/25\n",
            "31/31 [==============================] - 14s 457ms/step - loss: 0.6841 - accuracy: 0.5522 - val_loss: 0.6705 - val_accuracy: 0.5759\n",
            "Epoch 9/25\n",
            "31/31 [==============================] - 14s 452ms/step - loss: 0.6735 - accuracy: 0.5955 - val_loss: 0.6797 - val_accuracy: 0.6027\n",
            "Epoch 10/25\n",
            "31/31 [==============================] - 15s 483ms/step - loss: 0.6714 - accuracy: 0.5791 - val_loss: 0.6570 - val_accuracy: 0.6205\n",
            "Epoch 11/25\n",
            "31/31 [==============================] - 14s 455ms/step - loss: 0.6658 - accuracy: 0.5846 - val_loss: 0.6476 - val_accuracy: 0.6473\n",
            "Epoch 12/25\n",
            "31/31 [==============================] - 14s 453ms/step - loss: 0.6508 - accuracy: 0.6378 - val_loss: 0.6462 - val_accuracy: 0.6384\n",
            "Epoch 13/25\n",
            "31/31 [==============================] - 14s 457ms/step - loss: 0.6357 - accuracy: 0.6455 - val_loss: 0.6485 - val_accuracy: 0.6250\n",
            "Epoch 14/25\n",
            "31/31 [==============================] - 15s 481ms/step - loss: 0.6388 - accuracy: 0.6417 - val_loss: 0.6183 - val_accuracy: 0.6652\n",
            "Epoch 15/25\n",
            "31/31 [==============================] - 16s 516ms/step - loss: 0.6292 - accuracy: 0.6644 - val_loss: 0.6053 - val_accuracy: 0.6696\n",
            "Epoch 16/25\n",
            "31/31 [==============================] - 14s 461ms/step - loss: 0.6479 - accuracy: 0.6289 - val_loss: 0.6226 - val_accuracy: 0.7098\n",
            "Epoch 17/25\n",
            "31/31 [==============================] - 14s 445ms/step - loss: 0.6147 - accuracy: 0.6654 - val_loss: 0.6135 - val_accuracy: 0.6830\n",
            "Epoch 18/25\n",
            "31/31 [==============================] - 14s 451ms/step - loss: 0.6132 - accuracy: 0.6654 - val_loss: 0.6095 - val_accuracy: 0.6964\n",
            "Epoch 19/25\n",
            "31/31 [==============================] - 14s 439ms/step - loss: 0.6185 - accuracy: 0.6585 - val_loss: 0.5897 - val_accuracy: 0.6786\n",
            "Epoch 20/25\n",
            "31/31 [==============================] - 15s 470ms/step - loss: 0.5987 - accuracy: 0.7031 - val_loss: 0.5900 - val_accuracy: 0.7054\n",
            "Epoch 21/25\n",
            "31/31 [==============================] - 14s 446ms/step - loss: 0.5925 - accuracy: 0.6895 - val_loss: 0.6234 - val_accuracy: 0.6696\n",
            "Epoch 22/25\n",
            "31/31 [==============================] - 15s 482ms/step - loss: 0.5953 - accuracy: 0.7021 - val_loss: 0.5634 - val_accuracy: 0.6875\n",
            "Epoch 23/25\n",
            "31/31 [==============================] - 14s 448ms/step - loss: 0.6040 - accuracy: 0.6870 - val_loss: 0.6241 - val_accuracy: 0.6384\n",
            "Epoch 24/25\n",
            "31/31 [==============================] - 14s 464ms/step - loss: 0.5764 - accuracy: 0.6992 - val_loss: 0.5663 - val_accuracy: 0.7054\n",
            "Epoch 25/25\n",
            "31/31 [==============================] - 14s 450ms/step - loss: 0.5646 - accuracy: 0.7109 - val_loss: 0.5919 - val_accuracy: 0.6875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f00cc4e0df0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "batch_size = 32\n",
        "# image augmentation part\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# create training set\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/ML/datasetv2/dataset/training_set',\n",
        "                                                 target_size = input_size,\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "# create test set\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/ML/datasetv2/dataset/test_set',\n",
        "                                            target_size = input_size,\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "# fit the cnn model to the trainig set and testing it on the test set\n",
        "model.fit(training_set,\n",
        "          steps_per_epoch = 1000/batch_size,\n",
        "          epochs = 25,\n",
        "          validation_data = test_set,\n",
        "          validation_steps = 200/batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FablVLKI1ujz"
      },
      "source": [
        "## PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zkpQ9_Bbdgug"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OARxcBiKqhaP",
        "outputId": "3f2eff7a-51d5-4213-caf3-3373f1ab045a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 30ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "test_image = image.load_img('/content/drive/MyDrive/ML/datasetv2/dataset/predict/cat_or_dog_1.jpg', target_size= input_size)\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = model.predict(test_image)\n",
        "\n",
        "training_set.class_indices\n",
        "if result [0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'\n",
        "\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting"
      ],
      "metadata": {
        "id": "Lkp95apG0k9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "input_size = (128, 128)\n",
        "\n",
        "# Adding more Convolutional layer and MaxPooling layer\n",
        "model.add(tf.keras.layers.Conv2D(32, (3, 3), input_shape=(*input_size, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# FLATTENING LAYER\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Thêm các lớp Dense và Dropout\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "batch_size = 32\n",
        "# image augmentation part\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# create training set\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/ML/datasetv2/dataset/training_set',\n",
        "                                                 target_size = input_size,\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 class_mode = 'binary')\n",
        "# create test set\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/ML/datasetv2/dataset/test_set',\n",
        "                                            target_size = input_size,\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode = 'binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMoARocp2Dug",
        "outputId": "ba2bf04d-f129-4a78-cf12-47e2f07496e7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2008 images belonging to 2 classes.\n",
            "Found 392 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add callbacks for Early Stopping and Model Checkpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# fit the cnn model to the trainig set and testing it on the test set\n",
        "model.fit(training_set,\n",
        "          steps_per_epoch = 1000/batch_size,\n",
        "          epochs = 25,\n",
        "          validation_data = test_set,\n",
        "          validation_steps = 200/batch_size,\n",
        "          callbacks=[early_stopping, model_checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9V3wJ9a65ya",
        "outputId": "baea44c3-e658-444d-e1c7-c054225108ba"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "31/31 [==============================] - 63s 2s/step - loss: 0.6976 - accuracy: 0.5069 - val_loss: 0.6930 - val_accuracy: 0.5045\n",
            "Epoch 2/25\n",
            "31/31 [==============================] - 54s 2s/step - loss: 0.6934 - accuracy: 0.5127 - val_loss: 0.6896 - val_accuracy: 0.5000\n",
            "Epoch 3/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.6941 - accuracy: 0.5078 - val_loss: 0.6919 - val_accuracy: 0.5045\n",
            "Epoch 4/25\n",
            "31/31 [==============================] - 49s 2s/step - loss: 0.6901 - accuracy: 0.5156 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
            "Epoch 5/25\n",
            "31/31 [==============================] - 52s 2s/step - loss: 0.6848 - accuracy: 0.5605 - val_loss: 0.6721 - val_accuracy: 0.6384\n",
            "Epoch 6/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.6896 - accuracy: 0.5433 - val_loss: 0.6732 - val_accuracy: 0.5402\n",
            "Epoch 7/25\n",
            "31/31 [==============================] - 52s 2s/step - loss: 0.6733 - accuracy: 0.5957 - val_loss: 0.6554 - val_accuracy: 0.5893\n",
            "Epoch 8/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.6705 - accuracy: 0.5994 - val_loss: 0.6485 - val_accuracy: 0.6339\n",
            "Epoch 9/25\n",
            "31/31 [==============================] - 52s 2s/step - loss: 0.6388 - accuracy: 0.6427 - val_loss: 0.6581 - val_accuracy: 0.5804\n",
            "Epoch 10/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.6523 - accuracy: 0.6132 - val_loss: 0.5885 - val_accuracy: 0.7277\n",
            "Epoch 11/25\n",
            "31/31 [==============================] - 52s 2s/step - loss: 0.6001 - accuracy: 0.6703 - val_loss: 0.5937 - val_accuracy: 0.7054\n",
            "Epoch 12/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.6163 - accuracy: 0.6621 - val_loss: 0.6054 - val_accuracy: 0.7098\n",
            "Epoch 13/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.6098 - accuracy: 0.6772 - val_loss: 0.5908 - val_accuracy: 0.6920\n",
            "Epoch 14/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.6010 - accuracy: 0.6826 - val_loss: 0.5449 - val_accuracy: 0.7098\n",
            "Epoch 15/25\n",
            "31/31 [==============================] - 52s 2s/step - loss: 0.5770 - accuracy: 0.7031 - val_loss: 0.5319 - val_accuracy: 0.7009\n",
            "Epoch 16/25\n",
            "31/31 [==============================] - 52s 2s/step - loss: 0.5742 - accuracy: 0.6895 - val_loss: 0.5695 - val_accuracy: 0.6920\n",
            "Epoch 17/25\n",
            "31/31 [==============================] - 53s 2s/step - loss: 0.5594 - accuracy: 0.7254 - val_loss: 0.4471 - val_accuracy: 0.7991\n",
            "Epoch 18/25\n",
            "31/31 [==============================] - 53s 2s/step - loss: 0.5309 - accuracy: 0.7529 - val_loss: 0.5115 - val_accuracy: 0.7143\n",
            "Epoch 19/25\n",
            "31/31 [==============================] - 49s 2s/step - loss: 0.5311 - accuracy: 0.7618 - val_loss: 0.5412 - val_accuracy: 0.7232\n",
            "Epoch 20/25\n",
            "31/31 [==============================] - 52s 2s/step - loss: 0.5019 - accuracy: 0.7500 - val_loss: 0.5521 - val_accuracy: 0.7098\n",
            "Epoch 21/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.5298 - accuracy: 0.7333 - val_loss: 0.4872 - val_accuracy: 0.7679\n",
            "Epoch 22/25\n",
            "31/31 [==============================] - 50s 2s/step - loss: 0.4822 - accuracy: 0.7715 - val_loss: 0.5078 - val_accuracy: 0.7679\n",
            "Epoch 23/25\n",
            "31/31 [==============================] - 52s 2s/step - loss: 0.5031 - accuracy: 0.7617 - val_loss: 0.5681 - val_accuracy: 0.7098\n",
            "Epoch 24/25\n",
            "31/31 [==============================] - 51s 2s/step - loss: 0.4664 - accuracy: 0.7756 - val_loss: 0.4509 - val_accuracy: 0.7991\n",
            "Epoch 25/25\n",
            "31/31 [==============================] - 51s 2s/step - loss: 0.4561 - accuracy: 0.7953 - val_loss: 0.4981 - val_accuracy: 0.7589\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f00c65cba00>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  prediction\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "test_image = image.load_img('/content/drive/MyDrive/ML/datasetv2/dataset/predict/cat_or_dog_1.jpg', target_size=input_size)\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "result = model.predict(test_image)\n",
        "\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'dog'\n",
        "else:\n",
        "    prediction = 'cat'\n",
        "\n",
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HAO9pmlHCuqv",
        "outputId": "19ec054c-07b9-43e8-c0c3-2e35049ac202"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0MhwKS27bfO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}